用 wordcloud 生成 指定 形状 的 词 云图 wordcloud 是 python 扩展 库 中一 种 将 词 语用 图片 表达出来 的 一种 形式 通过 词 云 生成 的 图片 我们 可以 更加 直观 的 看出 某 篇文章 的 故事梗概 首先 贴出 一张 词 云图 以 哈利 波特 小说 为 例 在 生成 词 云图 之前 首先 要做 一些 准备工作 安装 结巴 分 词库 中 的 分词 模块 有 很多 他们 的 功能 也 都是 大同小异 我们 安装 的 结巴 分词 是 当前 使用 的 最多 的 类型 下面 我 来 简单 介绍 一下 结巴 分词 的 用法 结巴 分词 的 分词 模式 分为 三种 全 模式 把 句子 中所 有的 可以 成 词 的 词语 都 扫描 出来 速度快 但是 不能解决 歧义 问题 精确 模式 将 句子 最 精确地 切开 适合 文本 分析 搜索引擎 模式 在 精确 模式 的 基础上 对 长词 再次 切分 提高 召回 率 适合 用于 搜索引擎 分词 下面 用 一个 简单 的 例子 来看 一下 三种 模式 的 分词 区别 importjieba 全 模式 把 句子 中所 有的 可以 成 词 的 词语 都 扫描 出来 速度快 但是 不能解决 歧义 问题 text 哈利 波特 是 一 常 优秀 的 文学作品 seglistjiebacut textcutalltrue print u 全 模式 join seglist 精确 模式 将 句子 最 精确地 切开 适合 文本 分析 seglistjiebacut textcutallfalse print u 精确 模式 join seglist 默认 是 精确 模式 seglistjiebacut text print u 默认 模式 join seglist 搜索引擎 模式 在 精确 模式 的 基础上 对 长词 再次 切分 提高 召回 率 适合 用于 搜索引擎 分词 text print u 搜索引擎 模式 join seglist 下面 是 对 这 句话 的 分词 方式 通过 这三种 分词 模式 可以 看出 这些 分词 模式 并没有 很好 的 划 分出 哈利 波特 这个 专有名词 这是 因为 在 结巴 分词 的 字典 中 并没有 记录 这个 名词 所以 需要 我们 手动 添加 自定义 字典 添加 自定义 字典 找 一个 方便 引用 的 位置 下图 的 路径 是 我 安装 的 位置 新建 文本文档 后缀 名为 txt 将 想 添加 的 词 输入 进去 注意 输入 格式 保存 并 退出 在上 面的 代码 中 加入 自定义 字典 的 路径 再 点击 运行 分词 结果 可以 看出 哈利 波特 这个词 已经 被 识别 出来 了 结巴 分词 还有 另一个 禁 用词 的 输出 结果 优秀 文学作品 添加 禁 用词 之后 seglistjiebacut text final print u 切割 之后 join seglistnew 可以 看到 输出 结果 中 并没有 优秀 和 文学作品 两个 词 结巴 分词 还有 很多 比较复杂 的 操作 具体 的 可以 去官 网 查看 我 就不 再 过多 的 赘述 了 下面 我们 正式 开始 词 云 的 制作 首先 下载 模块 这里 我 所 使用 的 环境 是 anaconda 由于 anaconda 中 包含 很多 常用 的 扩展 包 所以 这里 只需要 下载 wordcloud 若 使用 的 环境 不是 anaconda 则 另 需 安装 numpy 和 pil 模块 然后 我们 需要 找 一篇 文章 并 使用 结巴 分词 将 文章 分成 词语 的 形式 分词 模块 defcut text 选择 分词 模式 textcutalltrue 分词 后 在 单独 个体 之间 加上 空格 resultjoin wordlist 返回 分词 结果 returnresult 这里 我 在当 前文 件 夹下 创 建了 一个 文本文档 xiaoshuotxt 并 复制 了 一章 的 小说 作为 词 云 的 主体 文字 使用 代码 控制 打开 并 读取 小说 的 内容 导入 文本文件 进行 分词 制 作词 云 withopen xiaoshuotxt asfptextfpread 将 读取 的 中文 文档 进行 分词 textcut text 在 网上 找到 一张 白色 背景 的 图片下载 到 当前 文件夹 作为 词 云 的 背景图 若不 指定 图片 则 默认 生成 矩形 词 云 设置 词 云 形状 若 设置 了 词 云 的 形状 生成 的 词 云 与 图片 保持一致 后面 设置 的 宽度 和 高度 将 默认 无效 masknparray imageopen monkeyjpeg 接下来 可以 根据 喜 好来 定义 词 云 的 颜色 轮廓 等 参数 下面 为 常用 的 参数设置 方法 完整 代码 导入 词 云 库 导入 图像 处理 库 导入 数据处理 库 importnumpyasnp 导入 结巴 分 词库 importjieba 分词 模块 defcut text 选择 分词 模式 textcutalltrue 分词 后 在 单独 个体 之间 加上 空格 resultjoin wordlist returnresult 导入 文本文件 进行 分词 制 作词 云 withopen xiaoshuotxt asfptextfpread 将 读取 的 中文 文档 进行 分词 textcut text 设置 词 云 形状 masknparray imageopen monkeyjpeg 自定义词 云 遮罩 层 除 白色 背景 外 其余 图层 全部 绘制 之前 设置 的 宽 高 无效 maskmask 默认 黑色 背景 更 改为 白色 按照 比例 扩大 或 缩小 画布 scale 若想 生成 中文字体 需 添加 中文字体 路径 逐 浪 雅 宋体 otf generate text 返回 对象 保存 图片 wordcloudtofile newwordcloudjpg 显示 图像 注 若 想要 生成 以下 样式 的 词 云图 找到 的 图片 背景 必须 为 白色 或者 使用 photoshop 抠 图 替换成 白色 背景 否则 生成 的 词 云 为 矩形 我 的 词 云 原图 生成 的 词 云 图文 源 网络 仅供 学习 之用 如有 侵权 联系 删除 我 将 优质 的 技术文章 和 经验总结 都 汇集 在 了 我 的 公众 号 python 圈子里 在 学习 python 的 道路上 肯定会 遇见 困难 别 慌 我 这里有 一套 学习 资料 包含 本 电子书 个 教学 视频 涉及 python 基础 爬虫 框架 数据 分析 机器 学习 等 不怕 你 学不会 还有 学习 交流 群 一起 学习 进步 