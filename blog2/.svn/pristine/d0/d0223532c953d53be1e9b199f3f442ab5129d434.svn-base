python 爬虫 遇到 ip 被封 的 情况 怎么办 文 源 网络 仅供 学习 之用 如有 侵权 请 联系 删除 代理 服务 的 介绍 我们 在做 爬虫 的 过程中 经常 最初 爬虫 都 正常 运行 正常 爬 取 数据 一切 看起来 都是 美好 然而 一杯 茶 的 功夫 就 出现 了 错误 如 forbidden 错误 您 的 ip 访问 频率 太高 错误 或者 跳出 一个 验证码 让我们 输入 之后 解封 但 过 一会 又 出现 类似 情况 出现 这个 现象 的 原因 是因为 网站 采 取了 一些 反 爬 中 措施 如 服务器 检测 ip 在 单位 时间内 请求 次数 超过 某个 阀值 导致 称为 封 ip 为了 解决 此类 问题 代理 就 派 上了 用场 如 代理 软件 付费 代理 adsl 拨号 代理 以 帮助 爬虫 脱离 封 ip 的 苦海 测试 http 请求 及 响应 的 网站 地址 地址 这个 网站 能 测试 http 请 求和 响应 的 各种 信息 比如 cookieipheaders 和 登录 验证 等 且 支持 getpost 等 多种 方法 对 web 开发 和 测试 很有 帮助 它用 pythonflask 编写 是 一个 开源 项目 开源 地址 返回 信息 中 origin 的 字段 就是 客户端 的 ip 地址 即可 判断 是否 成功 伪装 ip 代理 的 设置 urllib 的 代理 设置 需要 认证 的 代理 使用 proxyhandler 设置 代理 传入 参数 创建 opener 对象 proxyhandler print responseread decode utf ereason requests 的 代理 设置 需要 认证 的 代理 print responsetext erroreargs selenium 的 代理 使用 使用 的 是 带 认证 代理 browserget print 使用 的 是 chromeget 在 scrapy 使用 代理 在 scrapy 的 中间件 里 免费 代理 ip 的 使用 我们 可以 从 互联网 中 获取 免费 的 代理 ip 如 西 刺 定义 代理 池 proxylist 随机 选择 一个 代理 proxylist print responsetext erroreargs 收费 代理 ip 的 使用 收费 代理 还是 很多 的如 西 刺 讯 代理 快 代理 大象 代理 在 requests 中 使用 收费 代理 importrequests 从 代理 服务 中 获取 一个 代理 print responsetext erroreargs 