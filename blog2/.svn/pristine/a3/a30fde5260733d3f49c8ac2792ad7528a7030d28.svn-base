pyhon 疫情 大数 据分析 三 新闻 信息 抓取 及 词 云 可视化 文本 聚 类 和 lda 主题 模型 文本 挖掘 思来想去 虽然 很忙 但 还是 挤时间 针对 这次 肺炎 疫情 写个 python 大数 据分析 系列 博客 包括 网络 爬虫 可视化 分析 gis 地图 显示 情感 分析 舆情 分析 主题 挖掘 威胁 情报 溯源 知识 图谱 预测 预警 及 ai 和 nlp 应用 等 希望 该 系列 线上 远程教学 对 您 有所 帮助 也 希望 早点 战胜 病毒 武汉 加油 湖北 加油 全国 加油 待到 疫情 结束 樱花 盛开 这座 英雄 的 城市 等 你们 来 首 先说 声 抱歉 最近 一直 忙着 学习 安全 知识 其他 系列 文章 更新 较慢 已经有 一些人 催 更 了 哈哈 言归正传 前文 分享 了 腾讯 疫情 实时 数据 抓取 结合 pyecharts 绘制地图 折线图 柱状图 这篇 文章 将 爬 取 疫情 相关 的 新闻 数据 接着 进行 中文 分词 处理 及 文本 聚 类 lda 主题 模型 分析 希望 这篇 可视化 分析 文章 对 您 有所 帮助 也 非常感谢 参考文献 中 老师 的 分享 一起 加油 战胜 疫情 如果 您有 想 学习 的 知识 或 建议 可以 给 作者 留言 代码 下载 地址 下载 地址 文章 目录 一 数据 抓取 二 中文 分词 及 高频词 统计 结巴 分词 基本 用法 获取 疫情 文本 高频词 三 wordcloud 可视化 分析 基本 用法 疫情 词 云 四 tfidf 计算 及 kmeans 文本 聚 类 tfidf 计算 文本 聚 类 五 主题词 层次 聚类分析 层次 聚 类 疫情 分析 六 lda 主题 分布 分析 lda 主题 模型 完整 代码 七 总结 同时 推荐 前面 作者 另外 五个 python 系列 文章 从 年 开始 作者 主要 写了 三个 python 系列 文章 分 别是 基础知识 网络 爬虫 和数 据分析 年 陆续 增加了 python 图像识别 和 python 人工智能 专栏 python 基础知识 系列 python 基础知识 学习 与 提升 python 网络 爬虫 系列 python 爬虫 之 数据 分析 系列 知识 图谱 web 数据 挖掘 及 nlppython 图像识别 系列 python 图像 处理 及 图像识别 python 人工智能 系列 python 人工智能 及 知识 图谱 实战 前文 阅读 pyhon 疫情 大数 据分析 一 腾讯 实时 数据 爬 取 matplotlib 和 seaborn 可视化 分析 全国 各地区 某省 各城市 新增 趋势 pyhon 疫情 大数 据分析 二 pyecharts 绘制 全国 各地区 某省 各城市 疫情 地图 及 可视化 分析 一 数据 抓取 本次 爬虫 目标 网站 是 中国 社会组织 公共 服务 平台 读者 可以 针对 不同 的 网站 进行 抓取 第一步 分析 网站 通过 浏览器 审查 元素 查看 源代码 并 获取 新闻 的 标题 url 时间 等 不同 网站 有 不同 分析 方法 本文 重点是 文本 挖掘 数据 抓取 仅 提供 一些 思路 第二步 进入 具体 的 新闻 页面 抓取 相关 的 文本 信息 idfontinfoplast text idfontinfotext 第三步 本 爬虫 存在 一个 技巧 每条 新闻 的 url 非常 相似 这里 仅 变换 参数 来 抓取 新闻 最初 一篇 新闻 最后 一篇 新闻 我们 只需要 每次 抓取 数据 时 通过 下一 页 定位 下次 需要 抓取 的 url 即可 核心 代码 为 第四步 数据 抓取 完整 代码 如下 所示 建议 读者 直接 下载 我 的 文本 进行 数据 分析 而 不用 再去 爬 取 数据 高 访问 对方 数据库 记录 起始 时间 创建 csv 文件 并 写入 表头 信息 fpopen 中国 社会组织 疫情 防 控 writercsvwriter fp writerwriterow 标题 时间 url 正文 内容 来源 抓取 数据 url chromechrome 浏览器 随机 代理 response 获取 下一 页 链接 先 其他 元素 获取 一页 链接 保证 程序 的 强壮 性 print nexturlnexturl 获取 文章 标题 titlejoin articletitle texthtmlxpath print titletitle 获取 发布 时间 print print urlurl 获取 来源 idfontinfoplast text 爬 取 文本 idfontinfotext articletextjoin textlist replace rn replace xa replace t replace sourcetext replace title print print sourcesource writerwriterow 获取 结束时 的 时间 endtimetimetime usetime print 该 次 所获 的 信息 一共 使用 s 分钟 usetime 正常 退出 程序 sysexit 主 函数 defmain 第一 篇文章 正在 爬 取 第 s 篇 counturl url 抓取 的 结果 如 下图 所示 读者 可能 需要 安装 扩展 包 lxml 和 fakeuseragent 二 中文 分词 及 高频词 统计 结巴 分词 数据 预处理 是 指在 进行 数据 分析 之前 对 数据 进行 的 一些 初步 处理 包括 缺失 值 填写 噪声 处理 不一致 数据 修正 中文 分词 等 其 目标 是 得到 更 标准 高质量 的 数据 纠正 错误 异常 数据 从而 提升 分析 的 结果 中文 文本 预处理 的 基本 步骤 包括 中文 分词 词性 标注 数据 清洗 特征提取 向量 空间 模型 存储 权重 计算 tfidf 等 结巴 jieba 工具 是 最 常用 的 中文 文本 分词 和 处理 的 工具 之一 它能 实现 中文 分词 词性 标注 关键词 抽取 获取 词语 位置 等功能 其 在 github 网 站上 的 介绍 及 下载 地址 为 调用 命令 pipinstalljieba 安装 jieba 中文 分词 包 如 下图 所示 具有 以下 特点 支持 三种 分词 模式 包括 精确 模式 全 模式 和 搜索引擎 模式 支持 繁体 分词 支持 自定义 词典 代码 对 python 和 python 均 兼容 支持 多种 编程 语言 包括 等 基本 用法 首先 读者 看 一段 简单 的 结巴 分词 代码 主要 调用 两个 函数 实现 jiebacut textcutalltrue 分词 函数 第一个 参数 是 需要 分词 的 字符串 第二个 参数 表示 是否 为 全 模式 分词 返回 的 结果是 一个 可 迭代 的 生成器 generator 可 使用 for 循环 来 获取 分词 后 的 每个 词语 更 推荐 读者 转 换为 list 列表 再 使用 text 搜索引擎 模式 分词 参数 为 分词 的 字符串 该 方法 适合 用于 搜索引擎 构造 倒排 索引 的 分词 粒度 比较 细 小 杨 毕业 于 北京 理工大学 从事 python 人工智能 相关 工作 全 模式 datajiebacut textcutalltrue print type data print u 全 模式 join data 精确 模式 datajiebacut textcutallfalse print u 精确 模式 join data 默认 是 精确 模式 datajiebacut text print u 默认 模式 join data 搜索引擎 模式 text print u 搜索引擎 模式 join data 返回 列表 textcutallfalse print 返回 列表 format seglist 输出 结果 如 下图 所示 获取 疫情 文本 高频词 接着 我们 将 新闻 正文 文本 cclasstxt 数据 进行 中文 分词 每行 代表 一条 新闻 并 生成 对应 的 内容 中文 分词 cclassfencitxtw forlineinopen linestrip n seglistjiebacut linecutallfalse print join seglist cutwords join seglist fwrite cutwords 输出 结果 print allwords 词频 统计 ccounter x gtandxrncx 输出 词频 最高 的 前个 词 print n 词频 统计 结果 for kv incmostcommon print sd kv 存储 数据 ymd fccsvfwopen ifor kv incmostcommon len c fwwrite str i str k str v n iielseprint overwritefile fwclose 输出 结果 如 下图 所示 采用 空格 连接 的 分词 结果 同时 生成 高频 特征词 并 保存 至 csv 文件 中 对应 的 特征词 及 词频 排序 如表 fccsv 所示 如果 我们 撰写 图 情 论文 可以 尝试 建立 top 的 特征 词表 三 wordcloud 可视化 分析 基本 用法 词 云 分析 主要 包括 两种 方法 调用 wordcloud 扩展 包 画图 兼容性 极强 之前 介绍 过 调用 pyecharts 中 的 wordcloud 子 包 画图 本文 推荐 新方法 前 一篇 文章 作者 详细 讲 解了 pyecharts 可视化 相关 用法 这里 我们 需要 通过 它 来 绘制 词 云 基础 代码 如下 数据 words 背包 问题 大 整数 karatsuba 乘法 算法 穷举 搜索 傅里叶变换 状态 树 遍历 剪枝 galeshapley 最大 匹配 与 匈牙利 算法 线索 模型 关键 路径 算法 最小二乘法 曲线拟合 二分 逼近 法 牛顿 迭代法 bresenham 算法 粒 子群 优化 dijkstra a 算法 负 极大 极 搜索算法 估 值 函数 渲染 图 gtwordcloudc wordcloud add titlewordcloud 词 云 returnc 生成 图 wordcloudbase render 词 云图 html 输出 结果 如 下图 所示 出现 词频 越高 显示 越大 核心 代码 为 add namegtstr 图例 名称 attrgtlist 属性 名称 valuegtlist 属性 所 对应 的 值 shapegtlist 词 云图 轮廓 有 可选 wordgapgtint 单词 间隔 默 认为 单词 字体大小 范围 默 认为 rotatestepgtint 旋转 单词 角度 默 认为 疫情 词 云 接着 我们 显示 经过 中文 分词 的 疫情 新闻 文本 信息 前个 高频词 的 词 云 绘制 代码 如下 中文 分词 cclassfencitxtw forlineinopen linestrip n seglistjiebacut linecutallfalse print join seglist cutwords join seglist fwrite cutwords 输出 结果 print allwords 词频 统计 ccounter x gtandxrncx 输出 词频 最高 的 前个 词 print n 词频 统计 结果 for kv incmostcommon print sd kv 存储 数据 ymd fccsvfwopen ifor kv incmostcommon len c fwwrite str i str k str v n iielseprint overwritefile fwclose 词 云 分析 生成 数据 word a b c 列表 tuplewordsfor kv incmostcommon print kv wordsappend kv 渲染 图 gtwordcloudc wordcloud add setglobalopts title 全国 新型 冠状病毒 疫情 词 云图 returnc 生成 图 wordcloudbase render 疫情 词 云图 html 输出 结果 如 下图 所示 四 tfidf 计算 及 kmeans 文本 聚 类 tfidf 计算 是 一种 常 用于 信息处理 和 数据 挖掘 的 加权 技术 该 技术 采用 一种 统计 方法 根据 字词 的 在 文本 中 出现 的 次数 和在 整个 语料 中 出现 的 文档 频率 来 计算 一个 字词 在 整个 语 料中 的 重要 程度 它 的 优点 是 能 过滤掉 一些 常见 的 却 无关紧要 本 的 词语 同时 保留 影响 整个 文本 的 重要 字词 计算方法 如 下面 公式 所示 表示 某个 关键词 在 整篇文章 中 出现 的 频率 表示 计算 倒 文本 频率 文本 频率 是 指 某个 关键词 在 整个 语料 所有 文章 中 出现 的 次数 倒 文档 频率 又 称为 逆 文档 频率 它是 文档 频率 的 倒数 主要 用于 降低 所有 文档 中 一些 常见 却对 文档 影响 不大 的 词语 的 作用 tfidf 统计 可视化 完整 代码 如下 中文 分词 linestrip n seglistjiebacut linecutallfalse print join seglist cutwords join seglist userdicttxt 自定义 词典 stopwordstxt 停 用词 词典 提取 主题词 返回 的 词频 其实 就是 aennrnsv 词性 形容词 叹词 名词 动词 以 列表 形式 返回 print keywords 数据 存储 pddataframe keywordscolumns 词语 重要性 toexcel tfidf 关键词 前 xlsx keyword 本身 包含 两 列 数据 sspddataframe keywordscolumns 词语 重要性 print ss 数据 可视化 pltfigure figsize plttitle tfidfranking figpltaxes pltbarh range len ss 重要性 ss 重要性 figsetyticks nparange len ss 重要性 ss 词语 figsetxlabel importance pltshow 输出 结果 如 下图 所示 可以 看到 疫情 组织 捐赠 社会 协会 肺炎 物资 等 都是 高频词 也是 大众 普遍 关心 的 主题 注意 可能 需要 安装 openpyxl 扩展 包 toexcel 函数 要 用到 文本 聚 类 同样在 scikitlearn 包 中 也 能 计算 tfidf 权重 值 此时 需要 用到 两个 类 countvectorizer 和 类 会将 文本 中 的 词语 转 换为 词频 矩阵 例如 矩阵 中 包含 一个 元素 aij 它 表示 j 词 在 i 类 文本 下 的 词频 它 通过 fittransform 函数 计算 各个 词语 出现 的 次数 通过 getfeaturenames 可获 取词 袋 中所 有 文本 的 关键字 通过 toarray 可 看到 词频 矩阵 的 结果 用于 统计 vectorizer 中 每个 词语 的 tfidf 值 具体 用法 如下 语料 将 文本 中 的 词语 转 换为 词频 矩阵 计 算个 词语 出现 的 次数 corpus 获 取词 袋 中所 有 文本 关键词 printword 查看 词频 结果 printxtoarray 类 调用 将 词频 矩阵 x 统计 成 tfidf 值 x 查看 数据结构 tfidfij 表示 i 类 文本 中 的 tfidf 权重 输出 结果 入 下 所示 从 结果 中 可以 看到 总共 包括 个 特征词 即 同时 在 输出 每个 句子 中 包含 特征词 的 个数 例如 第一 句 它 对应 的 词频 为 假设 初始 序号 从 开始 计数 则 该 词频 表示 存在 第 个 位置 的 单词 document 共 次第 个 位置 的 单词 first 共 次第 个 位置 的 单词 is 共 次第 个 位置 的 单词 this 共 词 所以 每个 句子 都会 得到 一个 词频 向量 tfidf 对应 向量 类似 文本 聚 类 第一步 计算 tfidf 文档 预料 空格 连接 corpus 读取 预料 一行 预料 为 一个 文档 forlineinopen cclassfencitxtr readlines corpusappend linestrip 将 文本 中 的 词语 转 换为 词频 矩阵 矩阵 元素 aij 表示 j 词 在 i 类 文本 下 的 词频 该类 会 统计 每个 词语 的 tfidf 权 值 第一个 fittransform 是 计算 tfidf 第二个 fittransform 是 将 文本 转为 词频 矩阵 corpus 获 取词 袋 模型 中 的 所有 词语 将 tfidf 矩阵 抽取 出来 元素 wij 表示 j 词 在 i 类 文本 中 的 tfidf 权重 打印 特征向量 文本 内容 print len word 输出 单词 forjinrange len word print wordj 打印 每类 文本 的 tfidf 词语 权重 第一个 for 遍历 所有 文本 第二个 for 便利 某一 类 文本 下 的 词语 权重 foriinrange len weight printu 这里 输出 第 iu 类 文本 的 词语 tfidf 权重 forjinrange len word printweightij 第二步 聚 类 kmeansprint startkmeans nclusters print clf weight print pre 中心点 print print clfinertia 第三步 图形 输出 降 维 ncomponents 输出 两 维 weight 载入 n 维 print newdata xycpres pltlegend plttitle pltshow 输出 结果 如 下图 所示 需要 注意 简单 的 聚 类 我们 无法 进行 深入 的 分析 你 可以 理解为 积极 主题 的 一类 黄色 消极 主题 的 一类 黑色 也 可以 有 其他 理解 需要 结合 具体 数据 集 进行 分析 但 其 解释性 始终 不是 很好 而 真实 的 数据 分析 中会 引入 类 标 或 标注 所以 接着 我们 引入 主题 关键词 聚 类 和 lda 主题 模型 的 分析 更能 帮助 大家 理解 文本 挖掘 和 主题 分析 五 主题词 层次 聚类分析 层次 聚 类 层次 聚 类 算法 又 称为 树 聚 类 算法 它 根据 数据 之间 的 距离 透过 一种 层次 架构 方式 反复 将 数据 进行 聚合 创建 一个 层次 以 分解 给定 的 数据 集 主题词 层次 聚 类 主要 调用 实现 推荐 文章 层次 聚 类 层次 聚 类 编码 为 一个 linkage 矩阵 假设 代码 如下 z 共有 四列 组成 第一 字段 与 第二 字段 分别为 聚 类 簇 的 编号 在 初始 距离 前 每个 初始值 被 从 n 进行 标识 每 生成 一个 新 的 聚 类 簇 就 在此基础上 增加 一对 新 的 聚 类 簇 进行 标识 第三个 字段 表示 前 两个 聚 类 簇 之间 的 距离 第四个 字段 表示 新 生成 聚 类 簇 所 包含 的 元素 的 个数 x zlinkage xward ffcluster zdistance figpltfigure figsize dndendrogram z pltshow 下面 是 聚 类 结果 的 可视化 聚 类 树 下面 是 返回值 的 解析 疫情 分析 由于 层次 聚 类 绘制 的 树状 图 主题词 太多 所以 这里 采用 中文 分词 提取 每条 新闻 对应 一行 数据 的 top 特征词 再 存储 至 txt 中 进行 层次 聚类分析 完整 代码 如下 第一步 计算 top 计算 中文 分词 词频 linestrip n seglistjiebacut linecutallfalse print join seglist cutwords join seglist 输出 结果 print allwords 词频 统计 ccounter x gtandxrncx 输出 词频 最高 的 前个 词 topwordprint n 词频 统计 结果 for kv incmostcommon print sd kv topwordappend k print topword 疫情 防 控 组织工作 第二步 中文 分词 过滤 过滤 cutwordsfopen ckeytxtw forlineinopen linestrip n seglistjiebacut linecutallfalse finaln print cutwords fclose 第三步 相 相关 计算 textopen ckeytxt read listtextsplit n print list 数据 第一 行 第二 行 数据 print list print list 用于 删除 不 经常出现 的 术语 maxdf 用于 删除 过于 频繁 出现 的 术语 也 称为 语料库 特定 的 停 用词 mindfmaxdf mindf list toarray print len word print word print xxshape print xx titlesword 第四步 相似 度 计算 dfpddataframe xx print dfcorr print dfcorr spearman print dfcorr kendall distdfcorr print dist print type dist print distshape 第五步 可视化 分析 dist figsize treewordpngdpi 最终 生成 图像 如下 所示 运行 结果 如 下图 所示 注意 该 方法 更 推荐 大家 在 进行 论文 关键词 共 现 分析 主题词 聚类分析 等 领域 六 lda 主题 分布 分析 lda 主题 模型 文档 主题 生成 模型 简称 lda 通常 由 包含 词 主题 和 文档 三层 结构 组成 lda 模型 属于 无 监督 学习 技术 它是 将 一篇 文档 的 每个 词 都以 一定 概率 分布 在 某个 主 题上 并从 这个 主题 中 选择 某个 词语 文档 到 主题 的 过程 是 服从 多项 分布 的 主题 到 词 的 过程 也是 服从 多项 分布 的 文档 主题 生成 模型 简称 lda 又 称为 盘子 表示 法 platenotation 下图 是 模型 的 标示 图 其中 双 圆圈 表示 可 测 变量 单 圆圈 表示 潜在 变量 箭头 表示 两个 变量 之间 的 依赖 关系 矩形框 表示 重复 抽样 对应 的 重复 次数 在 矩形框 的 右下角 显示 lda 模型 的 具体 实现 步骤 如下 从 每篇 网页 d 对应 的 多项 分布 中 抽取 每个 单词 对应 的 一个 主题 z 从 主题 z 对应 的 多项 分布 中 抽取 一个 单词 w 重复 步骤 和 共计 nd 次 直至 遍历 网页 中 每一个 单词 读者 可以 从 gensim 中 下载 ldamodel 扩展 包 安装 也 可以 使用 sklearn 机器 学习 包 的 lda 子 扩展 包 亦 可从 github 中 下载 开源 的 lda 工具 下载 地址 详见 列表 所示 来源 下载 地址 利用 命令 安装 扩展 包 函数 即 lda 原型 本文 和 之前 介绍 的 lda 算法 略有不同 它 主要 采用 sklearn 中 的 包 实现 主题 分布 研究 并 调用 pyldavis 绘制 相关 图形 安装 过程 如下 所示 完整 代码 第一步 读取 数据 已 分词 corpus 读取 预料 一行 预料 为 一个 文档 forlineinopen cclassfencitxtr readlines corpusappend linestrip 第二步 计算 tfidf 值 设置 特征 数 的 或 等 是 有之 与 可以 还是 比较 这里 一个 和 也 被 吗 于 中最 但是 图片 大家 一下 几天 还有 一看 哈哈哈哈 怎么 本来 发现 andinofthe 我们 一直 真的 一次 了 有些 已经 不是 这么 一一一天 这个 这种 一种 位于 之一 天空 没有 很多 有点 什么 五个 特别 maxdfmindf 去除 文档 内 出现 几率 过大 或 过小 的 词汇 corpus print tfshape print tf 第三步 lda 分析 设置 主题 数 ldafit tf 显示 主题 数 ldacomponents 几个 主题 就是 几行 多少个 关键词 就是 几列 print 计算 困惑 度 print u 困惑 度 print ldaperplexity 主题 关键词 分布 modelcomponents ldacomponent 相当于 topicdtopicidx print join ntopwords print 定义 好 函数 之后 暂定 每个 主题 输出 前个 关键词 调用 函数 printtopwords 第四步 可视化 分析 print data 显示 图形 pyldavisshow data datafileobjhtml 困惑 度 及 各个 主 题下 的 关键词 通过 for 循环 显示 如下 topic 是 疫情 相关 的 主题词 topic 是 其他 相关 的 主题 困惑 度 topic 思源 工程 新华书店 中华 咨询师 供应商 扶贫 南安市 贵州省 旅游 新华 名称 主席 秘书 长清 关 服务项目 万双 联系人 理事长 资讯 topic 疫情 防 控 组织 社会工作 协会 捐赠 企业 服务 万元 物资 社区 慈善 积极 会员 做好 肺炎 防疫 商会 口罩 生成 对应 的 图形 浏览器 会 打开 如 下图 所示 注意 lda 主题 分布 分析 需要 设置 不同 的 主题 值 这里是 也 可 以是 等等 那么 如何 确定 最佳 主题 数 呢 困惑 数 又有 什么用 呢 如果 存在 语义 知识 又 怎么 处理 呢 主题 如 何能 更加 准确 定位 呢 读者 可以 带着 这些 思考 去 探索 加油 七 总结 写到 这里 第三篇 疫情 分析 的 文章 就 讲解 完毕 希望 对 您 有所 帮助 尤其是 想 写 文本 挖掘 论文 的 读者 主要 包括 两部分 内容 实时 数据 爬 取 中文 文本 分词 及 高频词 提 取词 云 可视化 分析 tfidf 权重 计算 和 文本 聚类分析 层次 聚类分析 lda 主题 模型 分布 后续 还会 分享 情感 分析 舆情 分析 主题 挖掘 威胁 情报 溯源 知识 图谱 预测 预警 及 ai 和 nlp 应用 等 如果 文章 对 您 有所 帮助 将是 我 写作 的 最大 动力 作者 将 源代码 上传 至 github 大家 可以 直接 下载 你们 的 支持 就是 我 撰写 的 最大 动力 加油 同时 向 钟 院士 致敬 向 一线 工作者 致敬 侠之大者 为国为民 咱们 中国人 一生 的 最高 追求 为 天地 立 心 为 生民 立 命为 往 圣 继 绝学 为 万世 开 太平 以 一人 之 力系 万民 康乐 以 一身 犯 险 保 大业 安全 他们 真是 做 到了 武汉 加油 中国 加油 byeastmount 中午 点 于 贵阳 参考文献 python 数据 挖掘 课程 十三 wordcloud 词 云 配置 过程 及 词频 分析 处理 文档 主题 分布 代码 入门 笔记 文本 聚 类 算法 pac 降 维 matplotlib 显示 聚 类 图像 eastmountpython 使用 jieba 工具 中文 分词 及 文本 聚 类 概念 eastmount 原创 博 文教 你 使用 pyecharts 绘制 词 云图 浮世 若 离 用 pyecharts 绘制 词 云 用 绘制 图形 二 折线图 折线 面积 图 散点图 雷达 图 箱 线图 词 云图 蒜泥 的 冬天 python 数据 挖掘 课程 二十八 基于 lda 和 pyldavis 的 主题 挖掘 及 可视化 分析 eastmountpython 层次 聚 类 函数 详解 tanhandsome 